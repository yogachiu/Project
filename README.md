# 低品質交通號誌影像辨識系統效能提昇與優化之研究
## 研究動機
提及現代智能交通系統，不論是多模態方案或純視覺方案，對於號誌的辨識都極為重要。儘管影像技術的發展已經在各個領域取得了顯著的成就，但在面對低品質影像時，仍然存在諸多挑戰。低品質影響因解析度低、光線不足、天氣影響（如霧氣、雨水）仍為圖像復原和辨識帶來極大的挑戰。

## 參與內容
- 文獻閱讀與整理
- 實驗環境架設
- Cycle-SNSPGAN 模型訓練與測試、成效分析
- ConvNeXt 模型訓練與測試、成效分析
- 實驗結果整合分析

## 研究流程
![image](https://github.com/user-attachments/assets/1cdfb4b6-12b2-4a5a-930f-5c3d04dcddef)

## 資料集介紹
CURE-TSR（Challenging Unreal and Real Environments for Traffic Sign Recognition）由 Temel 等人於 2017 年建立，旨在評估交通號誌識別模型在多種惡劣條件下的穩健性。該資料集彌補了傳統交通標誌資料集在規模與挑戰多樣性上的不足，總共包含 超過 200 萬張交通號誌圖片，來自真實世界（Real）與 模擬環境（Unreal）圖像。  

資料集組成如下：
- 14種類型的城市常見交通標誌，包括限速、禁止停車、停止、讓路等
  ![image](https://github.com/user-attachments/assets/bccc1018-2908-410a-b2a8-7843effe2916)

- 這些標誌在12種不同的挑戰條件下展示，如去色、鏡頭模糊、編碼錯誤、變暗、鏡頭髒污、曝光、高斯模糊、噪聲、雨、陰影、雪、霧等
- 每種條件有五種不同的嚴重程度
- 資料集中亦包含一組未受損的清晰影像（Challenge-free），作為基準參考
  
![image](https://github.com/user-attachments/assets/bf59a4e6-a27a-40e0-8716-bc7937814c4c)

## 使用技術與方法
### 分類模型：ConvNeXt
#### 簡介：
- 由 Liu 等人（2022）提出
- 專為視覺辨識需求而設計
- ResNet模型與Transformer的設計原則，融合了ViT的巨觀與微觀層級的設計策略
- 現代化卷積設計、階層式結構、高效性能與簡單可擴展等特點

#### 模型架構圖：
整體架構從 224x224 的輸入圖片開始，經過多層 ConvNeXt Block，並在不同階段進行下採樣以提取高階特徵。每個 ConvNeXt Block 中，會先使用 Depthwise 卷積進行空間特徵處理，搭配 LayerNorm、GELU 激活函數、1x1 卷積、DropPath 與殘差連接等現代設計，提升模型的穩定性與表現力。

![image](https://github.com/user-attachments/assets/2f5c4f70-124c-40da-b6a5-69ee1e017613)



#### 實驗設計：  

前期的實驗中，我們先找出epoch數目與圖片input size的基準：
- Epoch次數：約在 500 次時模型就已收斂
- Input size：圖片大小為 56*56 時有較高的辨識準確度 

在分類的實驗裡，所有圖像皆出自 CURE-TSR，主要以多種方式組合訓練集。主要控制項目基於以下幾點：
- 全部測試：圖像未經預處理，內含challenge-free（無損影像）和其他損毀狀況，一同訓練
  
- HSV處理：由於 CURE-TSR 數據集中包含變暗、過度曝光、陰影等一系列與亮度相關的問題，我們使用OpenCV中的直方圖均衡化（Histogram Equalization）功能進行亮度調整，使圖像效果更加清晰
  
- Cycle-SNSPGAN 除霧模型：使用 CURE-TSR 的 haze 圖像進行模型訓練，並將其應用於所有損壞類型圖片的修復處理，觀察是否能有效提升後續辨識準確度
  
- 訓練圖片數量：CURE-TSR的整體圖片張數龐大，為了讓訓練進度加快，我們以 python 中的 random 函數去按照號誌比例去取得整體的5%、10%、20%做訓練的基準
  
- 填補黑/白：我們將尺寸小於20與56的圖做補邊處裡，把以上兩種狀況的圖片缺少的部分以白色/黑色填補到56，其餘尺寸的圖片以拉縮來處理大小
  
- Challenge Free：我們的比較對象Aisha Batool等人(2023)的測試結果中，評比對象的訓練和測試皆基於 challenge free 圖像。為了進行有效的比較，我們亦採用了相同的實驗設計

![image](https://github.com/user-attachments/assets/7a12b371-ff6f-4350-bb72-5ec6a1b8cccf)


## 實驗成果展示
#### （以下圖片皆可成功辨識）

![image](https://github.com/user-attachments/assets/2d13a09d-34e7-4db5-94e8-e358c95b58b7)

## 實驗結論與分析
#### 實驗結果：
- 使用未經過前處理的圖片訓練模型對全損毀圖像分類有較高的準確度
- 測試全損毀影像的準確率達 88.89%  (32,5715張數）
- 測試無損毀影像的準確率達 95.21%  (11,544張數）

![image](https://github.com/user-attachments/assets/825a54f5-c574-4734-b325-cf6625172641)

#### 與對標論文實驗比對：
- 綠色標記：本研究實驗中的最佳表現結果
- 藍色標記：Batool 等人於 2023 年論文中所提出的數據

![image](https://github.com/user-attachments/assets/6bb99b3c-cd53-4509-90d6-abfc8a143edc)

#### 分析討論：
- 在數據呈現上，我們發現控制圖像尺寸，比起其他控制項目，有更直觀的提升
- 加入前處理反而導致模型表現下降，數據量的完整性和原始特性可能比前處理來得更為關鍵
- 經過HSV處理的模型在亮度與對比度肉眼改善上表現佳，但實際測試辨識率會下降，我們推測是因為丟失圖片原始資訊對辨識上有所影響

## 個人心得
當初會選擇專題實作，是因為我對影像辨識很有興趣，也想藉這個機會實際踏入這個領域探索與學習。在這一年多的研究過程中，雖然遇到不少挑戰，但收穫的經驗與知識，遠遠超過了那些挫折帶來的困難。回想一開始對實驗主題還很迷惘，到後來一步步釐清方向、設定目標，再慢慢朝著目標前進，整個過程雖然辛苦，但也非常充實。機器學習對我而言原本是一個完全陌生的領域，因此在初期閱讀相關文獻時感到相當吃力。然而，隨著不斷接觸與練習，我漸漸能夠掌握閱讀的技巧，如今已能迅速理解論文的架構與重點。這不僅提升了我對新技術的理解能力，也培養了我獨立學習的能力。此外，在實作的過程中，我也學會了如何建立實驗環境、進行資料前處理、訓練與調整模型參數，並根據實驗結果進行成效分析，進一步理解理論與實作之間的差異。

除了技術能力的提升，團隊合作也是這段經歷中很重要的一部分。我們需要分工合作、彼此支援，遇到問題時一起討論、集思廣益，常常能激發出新的想法。期末報告撰寫與發表，也讓我提升了整理資料與表達想法的能力。這段專題研究的經歷讓我收穫滿滿，不僅在專業知識上有了顯著的提升，也讓我在團隊合作和問題解決方面有了很大的進步。期許未來的自己，能夠將這些寶貴的經驗與技能，靈活運用在工作中。






